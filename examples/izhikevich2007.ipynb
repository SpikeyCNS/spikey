{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd0c671e72284273e5be387b60af2ce98411d2fa29290b07b37da2690b81960f96b",
   "display_name": "Python 3.8.5 64-bit ('Python38')"
  },
  "metadata": {
   "interpreter": {
    "hash": "c671e72284273e5be387b60af2ce98411d2fa29290b07b37da2690b81960f96b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Rough replication of Izhikevich's 2007 paper,\n",
    "Solving the Distal Reward Problem through linkage of STDP and Dopamine Signaling\n",
    "\n",
    "https://www.izhikevich.org/publications/dastdp.pdf\n",
    "\n",
    "Eugene M. Izhikevich(2007) Solving the Distal Reward Problem through linkage of STDP and Dopamine Signaling. Cerebral Cortex Advance Access, Jan 13, 2007. doi:10.1093/cercor/bhl152"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spikey.snn import *\n",
    "from spikey.core import GenericLoop, RLCallback\n",
    "from spikey.games import Logic\n",
    "from spikey.viz import print_rates, print_common_action\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rand_state(Logic):\n",
    "    \"\"\"\n",
    "    A customization of the Logic game, sets the game state randomly in 0..N at each timestep.\n",
    "    \"\"\"\n",
    "    NECESSARY_KEYS = Logic.extend_keys({\"n_states\": \"Number of input groups.\"})\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.observation_space = list(range(self._n_states))\n",
    "\n",
    "    def _get_state(self) -> np.ndarray:\n",
    "        # Randomly sample a state integer on game start and at every update\n",
    "        return np.random.randint(self._n_states)"
   ]
  },
  {
   "source": [
    "## Classical Conditioning\n",
    "\n",
    "In the original experiment there are N distinct input neuron groups, all pointing towards a single output group. The goal is to condition the output neurons to fire heavily in response to certain input groups, while largely ignoring others. This is accomplished by rewarding the network when the desired input groups fire to strengthen that group's connections to the outputs.\n",
    "\n",
    "Converting this description for use in the framework is straightforward, but if it's your first time needs frame of reference.\n",
    "\n",
    "1. Divide experiment into network and game mechanics.\n",
    "In this experiment the game is very simple, for each step a state in 0..N is randomly chosen that corresponds to the input group that is to fire, see rand_state in the cell above. The network will handle its own topology, input firings and reward scheme.\n",
    "\n",
    "2. Set up network inputs. \n",
    "First we split the set of input neurons into N groups, each will fire at a set rate when its respective state is active. In Spikey we accomplish this with the RateMap input type, with its state_rate_map parameter as an ndarray of all zeros except the diagonal which is set to the desired firing rate(=.2). state_rate_map can be a dictionary, ndarray or any other object that will index the state, used as group_rates = state_rate_map[state]. In this case if the state = 0, then group_rates = [.2, 0, 0, ...] which means group 0 will fire at a rate of 20% and all other groups will remain quincient. RateMap automatically divides the set of inputs into groups based on the size of the group rates vector.\n",
    "\n",
    "3. Set the topology of the network.\n",
    "Here we have a single fully connected feedforward layer, with each input connected to each output. Using the Manual weight part, we specify the network topology as a matrix in the shape (n_inputs+n_body, n_body) with n_body = n_hidden + n_output. For our purposes this looks like,\n",
    "\n",
    "```\n",
    "n_neurons\n",
    "------------------\n",
    "|   connected    |        n_inputs\n",
    "- - - - - - - - -\n",
    "|   unconnected  |        n_neurons\n",
    "------------------\n",
    "```\n",
    "\n",
    "with connected = uniform(0, 1) and unconnected = 0.\n",
    "\n",
    "4. Setup reward scheme and network readout.\n",
    "In this experiment reward is given solely based on the game state and ignores the network output. Therefore the readout function was arbitrarily chosen to be the simplest possible, a threshold function. A custom rewarder was setup in the state below, giving reward when the states is in the list 0, 3, 6 or 9."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateRewarder(reward.template.Reward):\n",
    "    FIRE_STATES = [0, 3, 6, 9]\n",
    "    def __call__(self, state, action, state_next):\n",
    "        # Give reward when state in desired states\n",
    "        if state in self.FIRE_STATES:\n",
    "            return self._reward_mult\n",
    "        else:\n",
    "            return self._punish_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'n_episodes': 5,\n",
    "    'len_episode': 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES = 10\n",
    "N_INPUTS = 100\n",
    "N_NEURONS = 50\n",
    "N_OUTPUTS = N_NEURONS\n",
    "\n",
    "w_matrix = np.vstack((  # Feedforward, no hidden layers\n",
    "    np.random.uniform(0, .5, (N_INPUTS, N_NEURONS)),\n",
    "    np.zeros((N_NEURONS, N_NEURONS)),\n",
    "))\n",
    "\n",
    "# When state is 1 neuron group 1 fires, ...\n",
    "state_rate_map = np.zeros((N_STATES, N_STATES))\n",
    "for state in range(N_STATES):\n",
    "    state_rate_map[state, state] = .2\n",
    "\n",
    "class network_template(RLNetwork):\n",
    "    parts = {\n",
    "        \"inputs\": input.RateMap,\n",
    "        \"neurons\": neuron.Neuron,\n",
    "        \"synapses\": synapse.RLSTDP,\n",
    "        \"weights\": weight.Manual,\n",
    "        \"readout\": readout.Threshold,\n",
    "        \"rewarder\": StateRewarder,\n",
    "    }\n",
    "    keys = {\n",
    "        \"n_inputs\": N_INPUTS,\n",
    "        \"n_neurons\": N_NEURONS,\n",
    "        \"n_outputs\": N_OUTPUTS,\n",
    "        \"matrix\": w_matrix,\n",
    "        \"magnitude\": 1,\n",
    "        \"potential_decay\": .05,\n",
    "\n",
    "        \"n_states\": N_STATES,\n",
    "        \"refractory_period\": 0,\n",
    "        \"firing_threshold\": 8,\n",
    "\n",
    "        \"processing_time\": 100,\n",
    "        \"learning_rate\": .1,\n",
    "        \"max_weight\": 2,\n",
    "        \"stdp_window\": 100,\n",
    "\n",
    "        \"reward_mult\": 1,\n",
    "        \"punish_mult\": 0,\n",
    "        \"action_threshold\": .0,  # Does not matter\n",
    "\n",
    "        \"state_rate_map\": state_rate_map, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.47s\n0: 0.02 -> 0.03\n1: 0.02 -> 0.03\n2: 0.02 -> 0.03\n3: 0.02 -> 0.03\n4: 0.02 -> 0.03\n5: 0.02 -> 0.03\n6: 0.02 -> 0.03\n7: 0.02 -> 0.03\n8: 0.02 -> 0.03\n9: 0.02 -> 0.03\n"
     ]
    }
   ],
   "source": [
    "# Control, without learning\n",
    "training_loop = GenericLoop(network_template, rand_state, measure_rates=True, **training_params)\n",
    "training_loop.reset(**{'learning_rate': 0, 'n_episodes': 1})\n",
    "network, game, results, info = training_loop()\n",
    "\n",
    "print(f\"{training_loop.callback.results['total_time']:.2f}s\")\n",
    "print_rates(callback=training_loop.callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Firing states: [0, 3, 6, 9]\n6.33s\n0: 0.02 -> 0.24\n1: 0.02 -> 0.03\n2: 0.02 -> 0.04\n3: 0.02 -> 0.19\n4: 0.02 -> 0.03\n5: 0.02 -> 0.03\n6: 0.02 -> 0.21\n7: 0.02 -> 0.03\n8: 0.02 -> 0.02\n9: 0.02 -> 0.21\n"
     ]
    }
   ],
   "source": [
    "# Real test\n",
    "training_loop = GenericLoop(network_template, rand_state, measure_rates=True, **training_params)\n",
    "network, game, results, info = training_loop()\n",
    "\n",
    "print(\"Firing states:\", StateRewarder.FIRE_STATES)\n",
    "print(f\"{training_loop.callback.results['total_time']:.2f}s\")\n",
    "print_rates(callback=training_loop.callback)"
   ]
  },
  {
   "source": [
    "## Classical Conditioning with Ordinal Output\n",
    "\n",
    "On top of the last experiment, here network outputs are split into two groups. The networks output is equal to the highest firing group, eg if group 0 fires more than any other group the network outputs a 0. The network is conditioned to output a 0 for states 2, 3, 6 and 8 and a 1 otherwise.\n",
    "\n",
    "A variation of the population vector readout was used, defined in the cell below. The base population vector readout returns a relative firing rate per each input group, eg [.25, .75], our custom MaxGroup readout takes this output and returns the index of the max group, eg 0 or 1.\n",
    "\n",
    "A custom rewarder was used to reward the network when the correct group fires the most."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxGroup(readout.PopulationVector):\n",
    "    def __call__(self, output_spike_train: np.bool) -> np.float:\n",
    "        # Network reads out index of highest firing output group\n",
    "        population_vector = super().__call__(output_spike_train)\n",
    "        return np.argmax(population_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinalRewarder(reward.template.Reward):\n",
    "    A_STATES = [2, 3, 6, 8]\n",
    "    def __call__(self, state, action, state_next):\n",
    "        # Expect network to output A(0) when current state in states listed,\n",
    "        # otherwise B(0)\n",
    "        if action == (state in self.A_STATES):\n",
    "            return self._reward_mult\n",
    "        else:\n",
    "            return self._punish_mult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'n_episodes': 10,\n",
    "    'len_episode': 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES = 10\n",
    "N_INPUTS = 100\n",
    "N_NEURONS = 60\n",
    "N_OUTPUTS = N_NEURONS\n",
    "\n",
    "w_matrix = np.vstack((  # Feedforward, no hidden layers\n",
    "    np.random.uniform(0, 1, (N_INPUTS, N_NEURONS)),\n",
    "    np.zeros((N_NEURONS, N_NEURONS)),\n",
    "))\n",
    "\n",
    "# When state is 1 neuron group 1 fires, ...\n",
    "state_rate_map = np.zeros((N_STATES, N_STATES), dtype=float)\n",
    "for state in range(N_STATES):\n",
    "    state_rate_map[state, state] = .2\n",
    "\n",
    "class network_template(RLNetwork):\n",
    "    parts = {\n",
    "        \"inputs\": input.RateMap,\n",
    "        \"neurons\": neuron.Neuron,\n",
    "        \"synapses\": synapse.RLSTDP,\n",
    "        \"weights\": weight.Manual,\n",
    "        \"readout\": MaxGroup,\n",
    "        \"rewarder\": OrdinalRewarder,\n",
    "    }\n",
    "    keys = {\n",
    "        \"n_inputs\": N_INPUTS,\n",
    "        'n_neurons': N_NEURONS,\n",
    "        \"n_outputs\": N_OUTPUTS,\n",
    "        \"matrix\": w_matrix,\n",
    "        \"magnitude\": 1,\n",
    "        \"potential_decay\": .05,\n",
    "\n",
    "        \"n_states\": N_STATES,\n",
    "        \"refractory_period\": 0,\n",
    "        \"firing_threshold\": 8,\n",
    "\n",
    "        \"processing_time\": 100,\n",
    "        \"learning_rate\": .1,\n",
    "        \"max_weight\": 2,\n",
    "        \"stdp_window\": 100,\n",
    "\n",
    "        \"reward_mult\": 1,\n",
    "        \"punish_mult\": 0,\n",
    "        \"n_actions\": 2,\n",
    "\n",
    "        \"state_rate_map\": state_rate_map,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.38s\n0: 0.0203 -> B(1). counts=[9]\n1: 0.0204 -> B(1). counts=[4 7]\n2: 0.0200 -> A(0). counts=[12]\n3: 0.0208 -> A(0). counts=[4 3]\n4: 0.0202 -> A(0). counts=[10]\n5: 0.0198 -> B(1). counts=[1 7]\n6: 0.0199 -> A(0). counts=[13]\n7: 0.0202 -> B(1). counts=[2 5]\n8: 0.0207 -> B(1). counts=[4 7]\n9: 0.0199 -> B(1). counts=[12]\n"
     ]
    }
   ],
   "source": [
    "# Control, without learning\n",
    "training_loop = GenericLoop(network_template, rand_state, measure_rates=True, **training_params)\n",
    "training_loop.reset(params={'learning_rate': 0, 'n_episodes': 1})\n",
    "network, game, results, info = training_loop()\n",
    "\n",
    "print(f\"{training_loop.callback.results['total_time']:.2f}s\")\n",
    "print_common_action(callback=training_loop.callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A States: [2, 3, 6, 8]\n12.65s\nInitial Responses\n0: 0.0204 -> B(1). counts=[7]\n1: 0.0199 -> A(0). counts=[12]\n2: 0.0198 -> A(0). counts=[10]\n3: 0.0195 -> A(0). counts=[7 3]\n4: 0.0193 -> A(0). counts=[7]\n5: 0.0197 -> B(1). counts=[15]\n6: 0.0199 -> A(0). counts=[14]\n7: 0.0196 -> B(1). counts=[1 8]\n8: 0.0200 -> A(0). counts=[5 2]\n9: 0.0200 -> B(1). counts=[9]\n\nFinal Responses\n0: 0.0198 -> B(1). counts=[11]\n1: 0.0197 -> A(0). counts=[9 3]\n2: 0.0201 -> A(0). counts=[11]\n3: 0.0193 -> A(0). counts=[10]\n4: 0.0193 -> B(1). counts=[9]\n5: 0.0208 -> B(1). counts=[7]\n6: 0.0193 -> A(0). counts=[7]\n7: 0.0208 -> B(1). counts=[10]\n8: 0.0197 -> A(0). counts=[8]\n9: 0.0199 -> B(1). counts=[15]\n"
     ]
    }
   ],
   "source": [
    "# Real test\n",
    "training_loop = GenericLoop(network_template, rand_state, measure_rates=True, **training_params)\n",
    "network, game, results, info = training_loop()\n",
    "\n",
    "print(\"A States:\", OrdinalRewarder.A_STATES)\n",
    "print(f\"{training_loop.callback.results['total_time']:.2f}s\")\n",
    "print(\"Initial Responses\")\n",
    "print_common_action(callback=training_loop.callback, episode=0)\n",
    "print(\"\\nFinal Responses\")\n",
    "print_common_action(callback=training_loop.callback, episode=-1)"
   ]
  }
 ]
}