{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bit1466fe8d14ba450eaad73bc0bfdd77c3",
   "display_name": "Python 3.7.6 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Rough result replication of Izhikevich's 2007 paper,\n",
    "Solving the Distal Reward Problem through linkage of STDP and Dopamine Signaling\n",
    "\n",
    "https://www.izhikevich.org/publications/dastdp.pdf\n",
    "\n",
    "Eugene M. Izhikevich(2007) Solving the Distal Reward Problem through linkage of STDP and Dopamine Signaling. Cerebral Cortex Advance Access, Jan 13, 2007. doi:10.1093/cercor/bhl152"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from spikey.core import *\n",
    "from spikey.snn import *\n",
    "from spikey.RL import *\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rates(experiment_output, training_params):\n",
    "    _, __, ___, info = experiment_output\n",
    "\n",
    "    states = np.array(info['step_states'])\n",
    "    inrates = np.array(info['step_inrates'])\n",
    "    outrates = np.array(info['step_outrates'])\n",
    "\n",
    "    for state in range(10):\n",
    "        mean_inrates = np.mean(inrates[states == state])\n",
    "        mean_outrates = np.mean(outrates[states == state])\n",
    "\n",
    "        print(f\"{state}: {mean_inrates:.4f} -> {mean_outrates:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class game_template(Logic):\n",
    "    def _get_state(self) -> np.ndarray:\n",
    "        return np.random.randint(N_STATES)"
   ]
  },
  {
   "source": [
    "## Classical Conditioning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'n_episodes': 10,\n",
    "    'len_episode': 100,\n",
    "}\n",
    "\n",
    "N_INPUTS = 100\n",
    "N_NEURONS = 50\n",
    "N_OUTPUTS = N_NEURONS\n",
    "\n",
    "FIRE_STATES = [0, 3, 6, 9]  # States network should fire in\n",
    "\n",
    "w_matrix = np.vstack((  # Feedforward, single layer\n",
    "    np.random.uniform(0, .5, (N_INPUTS, N_NEURONS)),\n",
    "    np.zeros((N_NEURONS, N_NEURONS)),\n",
    "))\n",
    "w_matrix = np.ma.array(np.float16(w_matrix), mask=(w_matrix == 0), fill_value=0)\n",
    "\n",
    "state_rate_map = np.zeros((N_STATES, N_STATES), dtype=np.float)\n",
    "for state in FIRE_STATES:\n",
    "    state_rate_map[state, state] = .2\n",
    "\n",
    "class network_template(RLNetwork):\n",
    "    config = {\n",
    "        \"n_inputs\": N_INPUTS,\n",
    "        'n_neurons': N_NEURONS,\n",
    "        \"n_outputs\": N_OUTPUTS,\n",
    "        'matrix': w_matrix,\n",
    "        'magnitude': 1,\n",
    "        'potential_decay': .05,\n",
    "        'output_range': [0, 1],\n",
    "        'trace_decay': .04,\n",
    "\n",
    "        'input_pct_inhibitory': 0,\n",
    "        'neuron_pct_inhibitory': 0,\n",
    "        'prob_rand_fire': 0,\n",
    "        'refractory_period': 0,\n",
    "        'firing_threshold': 10,\n",
    "\n",
    "        'processing_time': 100,\n",
    "        'learning_rate': .01,\n",
    "        'max_weight': 5,\n",
    "        'stdp_window': 20,\n",
    "\n",
    "        'reward_mult': 1,\n",
    "        'punish_mult': 0,\n",
    "        'action_threshold': .010,  # Most critical, no reward recieved unless surpasses this threshold\n",
    "\n",
    "        'expected_value': lambda state: state in FIRE_STATES,\n",
    "        'state_rate_map': state_rate_map, \n",
    "    }\n",
    "    _template_parts = {\n",
    "        'inputs': input.RateMap,\n",
    "        'neurons': neuron.Neuron,\n",
    "        'synapses': synapse.RLSTDPET,\n",
    "        'weights': weight.Manual,\n",
    "        'readout': readout.Threshold,\n",
    "        'rewarder': reward.MatchExpected,\n",
    "        'modifiers': None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5.67s\n0: 0.0203 -> 0.0153\n1: 0.0000 -> 0.0000\n2: 0.0000 -> 0.0001\n3: 0.0199 -> 0.0140\n4: 0.0000 -> 0.0000\n5: 0.0000 -> 0.0001\n6: 0.0198 -> 0.0149\n7: 0.0000 -> 0.0000\n8: 0.0000 -> 0.0000\n9: 0.0198 -> 0.0136\n"
     ]
    }
   ],
   "source": [
    "# Control, without learning\n",
    "callback = RLCallback(**training_params, reduced=False, measure_rates=True)\n",
    "training_loop = GenericLoop(network_template, game_template, training_params)\n",
    "training_loop.reset(params={'learning_rate': 0, 'n_episodes': 2})\n",
    "e_output = training_loop(callback=callback)\n",
    "\n",
    "print(f\"{callback.results['total_time']:.2f}s\")\n",
    "print_rates(e_output, training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "36.65s\n0: 0.0202 -> 0.2571\n1: 0.0000 -> 0.0012\n2: 0.0000 -> 0.0012\n3: 0.0199 -> 0.2219\n4: 0.0000 -> 0.0009\n5: 0.0000 -> 0.0007\n6: 0.0200 -> 0.2394\n7: 0.0000 -> 0.0009\n8: 0.0000 -> 0.0015\n9: 0.0202 -> 0.2272\n"
     ]
    }
   ],
   "source": [
    "# Real test\n",
    "callback = RLCallback(**training_params, reduced=False, measure_rates=True)\n",
    "training_loop = GenericLoop(network_template, game_template, training_params)\n",
    "e_output = training_loop(callback=callback)\n",
    "\n",
    "print(f\"{callback.results['total_time']:.2f}s\")\n",
    "print_rates(e_output, training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}