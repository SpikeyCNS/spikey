{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bit1466fe8d14ba450eaad73bc0bfdd77c3",
   "display_name": "Python 3.7.6 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Rough result replication of Izhikevich's 2007 paper,\n",
    "Solving the Distal Reward Problem through linkage of STDP and Dopamine Signaling\n",
    "\n",
    "https://www.izhikevich.org/publications/dastdp.pdf\n",
    "\n",
    "Eugene M. Izhikevich(2007) Solving the Distal Reward Problem through linkage of STDP and Dopamine Signaling. Cerebral Cortex Advance Access, Jan 13, 2007. doi:10.1093/cercor/bhl152"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from spikey.snn import *\n",
    "from spikey.core import GenericLoop, RLCallback\n",
    "from spikey.RL import Logic\n",
    "from spikey.viz import print_rates, print_common_action\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = RLCallback(reduced=False, measure_rates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class rand_state(Logic):\n",
    "    NECESSARY_KEYS = Logic.extend_keys({\"n_states\": \"Number of input groups.\"})\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.observation_space = list(range(self._n_states))\n",
    "\n",
    "    def _get_state(self) -> np.ndarray:\n",
    "        # Randomly sample a state integer on game start and at every update\n",
    "        return np.random.randint(self._n_states)"
   ]
  },
  {
   "source": [
    "## Classical Conditioning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"expected_value\": \"[any] func(state) Correct response of logic gate to specific state.\"]"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "Logic.NECESSARY_KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[\"expected_value\": \"[any] func(state) Correct response of logic gate to specific state.\",\n",
       " 'n_states']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "rand_state.NECESSARY_KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'n_episodes': 5,\n",
    "    'len_episode': 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES = 10\n",
    "N_INPUTS = 100\n",
    "N_NEURONS = 50\n",
    "N_OUTPUTS = N_NEURONS\n",
    "\n",
    "w_matrix = np.vstack((  # Feedforward, no hidden layers\n",
    "    np.random.uniform(0, .5, (N_INPUTS, N_NEURONS)),\n",
    "    np.zeros((N_NEURONS, N_NEURONS)),\n",
    "))\n",
    "\n",
    "# When state is 1 neuron group 1 fires, ...\n",
    "state_rate_map = np.zeros((N_STATES, N_STATES))\n",
    "for state in range(N_STATES):\n",
    "    state_rate_map[state, state] = .2\n",
    "\n",
    "FIRE_STATES = [0, 3, 6, 9]\n",
    "def is_target_state(state):\n",
    "    # Expect network to output True when current state in states listed,\n",
    "    # otherwise False\n",
    "    return state in FIRE_STATES\n",
    "\n",
    "class network_template(RLNetwork):\n",
    "    parts = {\n",
    "        \"inputs\": input.RateMap,\n",
    "        \"neurons\": neuron.Neuron,\n",
    "        \"synapses\": synapse.RLSTDPET,\n",
    "        \"weights\": weight.Manual,\n",
    "        \"readout\": readout.Threshold,\n",
    "        \"rewarder\": reward.MatchExpected,\n",
    "    }\n",
    "    keys = {\n",
    "        \"n_inputs\": N_INPUTS,\n",
    "        \"n_neurons\": N_NEURONS,\n",
    "        \"n_outputs\": N_OUTPUTS,\n",
    "        \"matrix\": w_matrix,\n",
    "        \"magnitude\": 1,\n",
    "        \"potential_decay\": .05,\n",
    "\n",
    "        \"n_states\": N_STATES,\n",
    "        \"refractory_period\": 0,\n",
    "        \"firing_threshold\": 8,\n",
    "        \"trace_decay\": .1,\n",
    "\n",
    "        \"processing_time\": 100,\n",
    "        \"learning_rate\": .1,\n",
    "        \"max_weight\": 2,\n",
    "        \"stdp_window\": 100,\n",
    "\n",
    "        \"reward_mult\": 1,\n",
    "        \"punish_mult\": 0,\n",
    "        \"action_threshold\": .0,  # Makes network always output True\n",
    "\n",
    "        \"expected_value\": is_target_state,\n",
    "        \"state_rate_map\": state_rate_map, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5.61s\n0: 0.02 -> 0.03\n1: 0.02 -> 0.03\n2: 0.02 -> 0.03\n3: 0.02 -> 0.03\n4: 0.02 -> 0.03\n5: 0.02 -> 0.03\n6: 0.02 -> 0.03\n7: 0.02 -> 0.03\n8: 0.02 -> 0.03\n9: 0.02 -> 0.03\n"
     ]
    }
   ],
   "source": [
    "# Control, without learning\n",
    "training_loop = GenericLoop(network_template, rand_state, callback, **training_params)\n",
    "training_loop.reset(**{'learning_rate': 0, 'n_episodes': 1})\n",
    "network, game, results, info = training_loop()\n",
    "\n",
    "print(f\"{callback.results['total_time']:.2f}s\")\n",
    "print_rates(callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Firing states: [0, 3, 6, 9]\n28.00s\n0: 0.02 -> 0.33\n1: 0.02 -> 0.18\n2: 0.02 -> 0.18\n3: 0.02 -> 0.35\n4: 0.02 -> 0.15\n5: 0.02 -> 0.21\n6: 0.02 -> 0.35\n7: 0.02 -> 0.16\n8: 0.02 -> 0.17\n9: 0.02 -> 0.35\n"
     ]
    }
   ],
   "source": [
    "# Real test\n",
    "training_loop = GenericLoop(network_template, rand_state, callback, **training_params)\n",
    "network, game, results, info = training_loop()\n",
    "\n",
    "print(\"Firing states:\", FIRE_STATES)\n",
    "print(f\"{callback.results['total_time']:.2f}s\")\n",
    "print_rates(callback=callback)"
   ]
  },
  {
   "source": [
    "## Classical Conditioning with Ordinal Output"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class max_group(readout.PopulationVector):\n",
    "    def __call__(self, output_spike_train: np.bool) -> np.float:\n",
    "        # Network returns index of highest firing output group\n",
    "        population_vector = super().__call__(output_spike_train)\n",
    "        return np.argmax(population_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'n_episodes': 10,\n",
    "    'len_episode': 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES = 10\n",
    "N_INPUTS = 100\n",
    "N_NEURONS = 60\n",
    "N_OUTPUTS = N_NEURONS\n",
    "\n",
    "w_matrix = np.vstack((  # Feedforward, no hidden layers\n",
    "    np.random.uniform(0, 1, (N_INPUTS, N_NEURONS)),\n",
    "    np.zeros((N_NEURONS, N_NEURONS)),\n",
    "))\n",
    "\n",
    "# When state is 1 neuron group 1 fires, ...\n",
    "state_rate_map = np.zeros((N_STATES, N_STATES), dtype=np.float)\n",
    "for state in range(N_STATES):\n",
    "    state_rate_map[state, state] = .2\n",
    "\n",
    "A_STATES = [2, 3, 6, 8]\n",
    "def is_a_state(state):\n",
    "    # Expect network to output A(0) when current state in states listed,\n",
    "    # otherwise B(0)\n",
    "    return state in A_STATES\n",
    "\n",
    "class network_template(RLNetwork):\n",
    "    parts = {\n",
    "        \"inputs\": input.RateMap,\n",
    "        \"neurons\": neuron.Neuron,\n",
    "        \"synapses\": synapse.RLSTDPET,\n",
    "        \"weights\": weight.Manual,\n",
    "        \"readout\": max_group,\n",
    "        \"rewarder\": reward.MatchExpected,\n",
    "    }\n",
    "    keys = {\n",
    "        \"n_inputs\": N_INPUTS,\n",
    "        'n_neurons': N_NEURONS,\n",
    "        \"n_outputs\": N_OUTPUTS,\n",
    "        \"matrix\": w_matrix,\n",
    "        \"magnitude\": 1,\n",
    "        \"potential_decay\": .05,\n",
    "\n",
    "        \"n_states\": N_STATES,\n",
    "        \"refractory_period\": 0,\n",
    "        \"firing_threshold\": 8,\n",
    "        \"trace_decay\": .1,\n",
    "\n",
    "        \"processing_time\": 100,\n",
    "        \"learning_rate\": .1,\n",
    "        \"max_weight\": 2,\n",
    "        \"stdp_window\": 100,\n",
    "\n",
    "        \"reward_mult\": 1,\n",
    "        \"punish_mult\": 0,\n",
    "        \"n_actions\": 2,\n",
    "\n",
    "        \"expected_value\": is_a_state,\n",
    "        \"state_rate_map\": state_rate_map,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6.63s\n0: 0.0201 -> B(1). counts=[10]\n1: 0.0202 -> A(0). counts=[12]\n2: 0.0195 -> A(0). counts=[4]\n3: 0.0194 -> A(0). counts=[7]\n4: 0.0203 -> B(1). counts=[15]\n5: 0.0200 -> A(0). counts=[9 5]\n6: 0.0202 -> B(1). counts=[1 4]\n7: 0.0201 -> A(0). counts=[6 5]\n8: 0.0199 -> A(0). counts=[8 6]\n9: 0.0202 -> B(1). counts=[8]\n"
     ]
    }
   ],
   "source": [
    "# Control, without learning\n",
    "training_loop = GenericLoop(network_template, rand_state, callback, **training_params)\n",
    "training_loop.reset(params={'learning_rate': 0, 'n_episodes': 1})\n",
    "network, game, results, info = training_loop()\n",
    "\n",
    "print(f\"{callback.results['total_time']:.2f}s\")\n",
    "print_common_action(callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "A States: [2, 3, 6, 8]\n59.82s\nInitial Responses\n0: 0.0200 -> B(1). counts=[11]\n1: 0.0202 -> A(0). counts=[11  2]\n2: 0.0192 -> A(0). counts=[7]\n3: 0.0195 -> A(0). counts=[4 2]\n4: 0.0201 -> B(1). counts=[14]\n5: 0.0192 -> B(1). counts=[6]\n6: 0.0201 -> A(0). counts=[11  1]\n7: 0.0205 -> A(0). counts=[7 4]\n8: 0.0205 -> A(0). counts=[10]\n9: 0.0199 -> B(1). counts=[10]\n\nFinal Responses\n0: 0.0194 -> B(1). counts=[12]\n1: 0.0206 -> B(1). counts=[1 9]\n2: 0.0199 -> A(0). counts=[13]\n3: 0.0203 -> A(0). counts=[6 3]\n4: 0.0195 -> B(1). counts=[12]\n5: 0.0194 -> B(1). counts=[3 4]\n6: 0.0202 -> A(0). counts=[11]\n7: 0.0198 -> B(1). counts=[7]\n8: 0.0201 -> A(0). counts=[5]\n9: 0.0197 -> B(1). counts=[14]\n"
     ]
    }
   ],
   "source": [
    "# Real test\n",
    "training_loop = GenericLoop(network_template, rand_state, callback, **training_params)\n",
    "network, game, results, info = training_loop()\n",
    "\n",
    "print(\"A States:\", A_STATES)\n",
    "print(f\"{callback.results['total_time']:.2f}s\")\n",
    "print(\"Initial Responses\")\n",
    "print_common_action(callback=callback, episode=0)\n",
    "print(\"\\nFinal Responses\")\n",
    "print_common_action(callback=callback, episode=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}