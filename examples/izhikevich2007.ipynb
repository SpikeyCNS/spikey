{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37664bit1466fe8d14ba450eaad73bc0bfdd77c3",
   "display_name": "Python 3.7.6 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "Rough result replication of Izhikevich's 2007 paper,\n",
    "Solving the Distal Reward Problem through linkage of STDP and Dopamine Signaling\n",
    "\n",
    "https://www.izhikevich.org/publications/dastdp.pdf\n",
    "\n",
    "Eugene M. Izhikevich(2007) Solving the Distal Reward Problem through linkage of STDP and Dopamine Signaling. Cerebral Cortex Advance Access, Jan 13, 2007. doi:10.1093/cercor/bhl152"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from spikey.core import *\n",
    "from spikey.snn import *\n",
    "from spikey.RL import *\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = RLCallback(reduced=False, measure_rates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class game_template(Logic):\n",
    "    def _get_state(self) -> np.ndarray:\n",
    "        return np.random.randint(N_STATES)"
   ]
  },
  {
   "source": [
    "## Classical Conditioning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rates(experiment_output, training_params, episode=-1):\n",
    "    network, __, ___, info = experiment_output\n",
    "\n",
    "    # step_states = [[ep 0 states], [ep 1 states], ...]\n",
    "    states = np.array(info['step_states'][episode])\n",
    "    inrates = np.array(info['step_inrates'][episode])\n",
    "    outrates = np.array(info['step_outrates'][episode])\n",
    "\n",
    "    for state in range(10):\n",
    "        mean_inrates = np.mean(inrates[states == state])\n",
    "        mean_outrates = np.mean(outrates[states == state])\n",
    "\n",
    "        print(f\"{state}: {mean_inrates:.4f} -> {mean_outrates:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'n_episodes': 5,\n",
    "    'len_episode': 100,\n",
    "}\n",
    "\n",
    "N_INPUTS = 100\n",
    "N_NEURONS = 50\n",
    "N_OUTPUTS = N_NEURONS\n",
    "\n",
    "FIRE_STATES = [0, 3, 6, 9]  # States network should fire in\n",
    "\n",
    "w_matrix = np.vstack((  # Feedforward, single layer\n",
    "    np.random.uniform(0, .5, (N_INPUTS, N_NEURONS)),\n",
    "    np.zeros((N_NEURONS, N_NEURONS)),\n",
    "))\n",
    "w_matrix = np.ma.array(np.float16(w_matrix), mask=(w_matrix == 0), fill_value=0)\n",
    "\n",
    "state_rate_map = np.zeros((N_STATES, N_STATES), dtype=np.float)\n",
    "for state in range(N_STATES):\n",
    "    state_rate_map[state, state] = .2\n",
    "\n",
    "class network_template(RLNetwork):\n",
    "    keys = {\n",
    "        \"n_inputs\": N_INPUTS,\n",
    "        'n_neurons': N_NEURONS,\n",
    "        \"n_outputs\": N_OUTPUTS,\n",
    "        'matrix': w_matrix,\n",
    "        'magnitude': 1,\n",
    "        'potential_decay': .05,\n",
    "\n",
    "        'refractory_period': 0,\n",
    "        'firing_threshold': 8,\n",
    "        'trace_decay': .1,\n",
    "\n",
    "        'processing_time': 100,\n",
    "        'learning_rate': .1,\n",
    "        'max_weight': 2,\n",
    "        'stdp_window': 100,\n",
    "\n",
    "        'reward_mult': 1,\n",
    "        'punish_mult': 0,\n",
    "        'action_threshold': .0,  # Makes network output always True, so reward is only given when state in FIRE_STATES \n",
    "\n",
    "        'expected_value': lambda state: state in FIRE_STATES,\n",
    "        'state_rate_map': state_rate_map, \n",
    "    }\n",
    "    parts = {\n",
    "        'inputs': input.RateMap,\n",
    "        'neurons': neuron.Neuron,\n",
    "        'synapses': synapse.RLSTDPET,\n",
    "        'weights': weight.Manual,\n",
    "        'readout': readout.Threshold,\n",
    "        'rewarder': reward.MatchExpected,\n",
    "        'modifiers': None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\cole\\AppData\\Local\\Python38\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\cole\\AppData\\Local\\Python38\\lib\\site-packages\\numpy\\core\\_methods.py:168: RuntimeWarning: invalid value encountered in true_divide\n",
      "  ret = arr.dtype.type(ret / rcount)\n",
      "10.80s\n",
      "0: 0.0203 -> 0.0321\n",
      "1: 0.0204 -> 0.0325\n",
      "2: 0.0195 -> 0.0334\n",
      "3: 0.0197 -> 0.0288\n",
      "4: 0.0200 -> 0.0280\n",
      "5: 0.0195 -> 0.0317\n",
      "6: 0.0195 -> 0.0305\n",
      "7: 0.0196 -> 0.0313\n",
      "8: 0.0192 -> 0.0248\n",
      "9: 0.0195 -> 0.0296\n"
     ]
    }
   ],
   "source": [
    "# Control, without learning\n",
    "training_loop = GenericLoop(network_template, game_template, callback, **training_params)\n",
    "training_loop.reset(**{'learning_rate': 0, 'n_episodes': 2})\n",
    "network, game, results, info = training_loop()\n",
    "\n",
    "print(f\"{callback.results['total_time']:.2f}s\")\n",
    "print_rates((network, game, results, info), training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 3, 6, 9]\n29.00s\n0: 0.0204 -> 0.3521\n1: 0.0204 -> 0.1705\n2: 0.0202 -> 0.2000\n3: 0.0194 -> 0.3191\n4: 0.0199 -> 0.1638\n5: 0.0201 -> 0.1888\n6: 0.0193 -> 0.3352\n7: 0.0202 -> 0.1873\n8: 0.0196 -> 0.1625\n9: 0.0197 -> 0.3381\n"
     ]
    }
   ],
   "source": [
    "# Real test\n",
    "training_loop = GenericLoop(network_template, game_template, callback, **training_params)\n",
    "network, game, results, info = training_loop()\n",
    "\n",
    "print(FIRE_STATES)\n",
    "print(f\"{callback.results['total_time']:.2f}s\")\n",
    "print_rates((network, game, results, info), training_params)"
   ]
  },
  {
   "source": [
    "## Instrumental Conditioning"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_group_rates(experiment_output, training_params, episode=-1):\n",
    "    network, __, ___, info = experiment_output\n",
    "\n",
    "    # step_states = [[ep 0 states], [ep 1 states], ...]\n",
    "    states = np.array(info['step_states'][episode])\n",
    "    inrates = np.array(info['step_inrates'][episode])\n",
    "    step_actions = np.array(info['step_actions'][episode])\n",
    "\n",
    "    for state in range(10):\n",
    "        mean_inrates = np.mean(inrates[states == state])\n",
    "        actions, counts = np.unique(step_actions[states == state], return_counts=True)\n",
    "        action = actions[np.argmax(counts)]\n",
    "        print(f\"{state}: {mean_inrates:.4f} -> {['A', 'B', 'C'][int(action)]}({action})\", step_actions[states==state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class max_group(readout.PopulationVector):\n",
    "    def __call__(self, output_spike_train: np.bool) -> np.float:\n",
    "        return np.argmax(super().__call__(output_spike_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'n_episodes': 10,\n",
    "    'len_episode': 100,\n",
    "}\n",
    "\n",
    "N_INPUTS = 100\n",
    "N_NEURONS = 60\n",
    "N_OUTPUTS = N_NEURONS\n",
    "\n",
    "A_STATES = [2, 3, 6, 8]  # States where group A should be higher than B\n",
    "\n",
    "w_matrix = np.vstack((  # Feedforward, single layer\n",
    "    np.random.uniform(0, 1, (N_INPUTS, N_NEURONS)),\n",
    "    np.zeros((N_NEURONS, N_NEURONS)),\n",
    "))\n",
    "w_matrix = np.ma.array(np.float16(w_matrix), mask=(w_matrix == 0), fill_value=0)\n",
    "\n",
    "state_rate_map = np.zeros((N_STATES, N_STATES), dtype=np.float)\n",
    "for state in range(N_STATES):\n",
    "    state_rate_map[state, state] = .2\n",
    "\n",
    "class network_template(RLNetwork):\n",
    "    keys = {\n",
    "        \"n_inputs\": N_INPUTS,\n",
    "        'n_neurons': N_NEURONS,\n",
    "        \"n_outputs\": N_OUTPUTS,\n",
    "        'matrix': w_matrix,\n",
    "        'magnitude': 1,\n",
    "        'potential_decay': .05,\n",
    "\n",
    "        'refractory_period': 0,\n",
    "        'firing_threshold': 8,\n",
    "        'trace_decay': .1,\n",
    "\n",
    "        'processing_time': 100,\n",
    "        'learning_rate': .1,\n",
    "        'max_weight': 2,\n",
    "        'stdp_window': 100,\n",
    "\n",
    "        'reward_mult': 1,\n",
    "        'punish_mult': 0,\n",
    "        'n_actions': 2,\n",
    "\n",
    "        'expected_value': lambda state: [0, 1][state in A_STATES],\n",
    "        'state_rate_map': state_rate_map,\n",
    "    }\n",
    "    parts = {\n",
    "        'inputs': input.RateMap,\n",
    "        'neurons': neuron.Neuron,\n",
    "        'synapses': synapse.RLSTDPET,\n",
    "        'weights': weight.Manual,\n",
    "        'readout': max_group,\n",
    "        'rewarder': reward.MatchExpected,\n",
    "        'modifiers': None,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2, 3, 6, 8]\n12.44s\n0: 0.0198 -> A(0) [0 0 0 0 0 0 0 0 0 0 0]\n1: 0.0199 -> A(0) [0 1 0 0 0 0 0 0 0 0 0 0]\n2: 0.0200 -> B(1) [1 1 1 1 1 1 1]\n3: 0.0204 -> A(0) [0 0 0 0 0 0 0]\n4: 0.0199 -> A(0) [0 0 0 0 0 0 0]\n5: 0.0204 -> B(1) [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n6: 0.0201 -> A(0) [0 0 0 0 0 0 0]\n7: 0.0201 -> B(1) [1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n8: 0.0199 -> A(0) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n9: 0.0197 -> A(0) [0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Control, without learning\n",
    "training_loop = GenericLoop(network_template, game_template, callback, **training_params)\n",
    "training_loop.reset(params={'learning_rate': 0, 'n_episodes': 2})\n",
    "network, game, results, info = training_loop()\n",
    "\n",
    "print(A_STATES)\n",
    "print(f\"{callback.results['total_time']:.2f}s\")\n",
    "print_group_rates((network, game, results, info), training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2, 3, 6, 8]\n64.86s\nInitial Responses\n0: 0.0202 -> B(1) [0 0 1 1 1 1 1 1 1 1 1 1]\n1: 0.0199 -> B(1) [0 1 1 1 1 1 1]\n2: 0.0199 -> B(1) [1 1 1 1 1 1]\n3: 0.0196 -> A(0) [0 0 0 0 0 0 0 0 0]\n4: 0.0201 -> A(0) [0 0 0 1 0 0 1 0]\n5: 0.0198 -> B(1) [1 1 1 1 0 0 1 1 1 1 1 1 1 1]\n6: 0.0204 -> A(0) [0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n7: 0.0206 -> B(1) [1 1 1 1 1 1 1 1 1]\n8: 0.0199 -> A(0) [0 0 0 0 0 0 1 0 0 0 0 1 0 0]\n9: 0.0205 -> A(0) [0 0 0 0 0 1 0]\n\nFinal Responses\n0: 0.0194 -> B(1) [1 1 1 1 1 1 1 1 1 1 1 1 1]\n1: 0.0199 -> B(1) [1 1 1 1 1 1 1 1]\n2: 0.0195 -> A(0) [0 0 0 0 0 0 0 0 0]\n3: 0.0199 -> A(0) [0 0 0 0 0 0 0 0 0 0 0]\n4: 0.0200 -> B(1) [1 1 1 1 1 1 1 1 1 1 1]\n5: 0.0195 -> B(1) [1 1 1 1 1 1 1 1 1]\n6: 0.0204 -> A(0) [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n7: 0.0208 -> B(1) [1 1 1 1 1 1 1 1 1 1 1 1 1]\n8: 0.0209 -> A(0) [1 0]\n9: 0.0200 -> B(1) [1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Real test\n",
    "training_loop = GenericLoop(network_template, game_template, callback, **training_params)\n",
    "network, game, results, info = training_loop()\n",
    "\n",
    "print(A_STATES)\n",
    "print(f\"{callback.results['total_time']:.2f}s\")\n",
    "print(\"Initial Responses\")\n",
    "print_group_rates((network, game, results, info), training_params, 0)\n",
    "print(\"\\nFinal Responses\")\n",
    "print_group_rates((network, game, results, info), training_params, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}