{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replication of Florian 2007 XOR gate experiments.\n",
    "* Rate based input coding\n",
    "* Temporal pattern coding\n",
    "\n",
    "https://www.florian.io/papers/2007_Florian_Modulated_STDP.pdf\n",
    "\n",
    "Florian R (2007) Reinforcement Learning Through Modulation of Spike-Timing-Dependent Synaptic Plasticity. Neural Computation 19(6). https://doi.org/10.1162/neco.2007.19.6.1468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from spikey.snn import *\n",
    "from spikey.games import Logic\n",
    "from spikey.viz import print_rates\n",
    "\n",
    "from ray.train.torch import TorchTrainer\n",
    "from ray.air.config import ScalingConfig, RunConfig\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_w_diffs(original_w, final_w, layer_cutoff):\n",
    "    print(f\"{np.sum(original_w[:, :layer_cutoff]):.0f} -> {np.sum(final_w[:, :layer_cutoff]):.0f}\")\n",
    "    print(f\"{np.sum(original_w[:, layer_cutoff:]):.0f} -> {np.sum(final_w[:, layer_cutoff:]):.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_success(step_states, step_inrates, step_sysrates, step_outrates, training_params):\n",
    "    states = np.array(step_states).reshape((-1, 2))\n",
    "    inrates = np.array(step_inrates).reshape((-1))\n",
    "    sysrates = np.array(step_sysrates).reshape((-1))\n",
    "    outrates = np.array(step_outrates).reshape((-1))\n",
    "\n",
    "    HIGH = [[False, True], [True, False]]\n",
    "    LOW =  [[False, False], [True, True]]\n",
    "\n",
    "    relevant_timeframe = training_params['eval_steps'] // 4\n",
    "\n",
    "    high_rate = min([np.mean(outrates[np.all(states == state, axis=1)][-relevant_timeframe:]) for state in HIGH])\n",
    "    low_rate = max([np.mean(outrates[np.all(states == state, axis=1)][-relevant_timeframe:]) for state in LOW])\n",
    "\n",
    "    print(high_rate, low_rate)\n",
    "    florian_win = high_rate > low_rate + .02\n",
    "\n",
    "    correct = 0\n",
    "    for i in range(training_params['eval_steps']):\n",
    "        state = states[-i]\n",
    "        rate = outrates[-i]\n",
    "\n",
    "        if np.sum(state) % 2:\n",
    "            correct += int(rate > low_rate)\n",
    "        else:\n",
    "            correct += int(rate < high_rate)\n",
    "\n",
    "    florian_accuracy = correct / training_params['eval_steps']\n",
    "\n",
    "    print(f\"Florian - Win: {florian_win}, Accuracy: {florian_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlorianReward(spikey.snn.reward.template.Reward):\n",
    "    def __call__(self, state, action, state_next):\n",
    "        if sum(state) % 2 == 1:  # (0, 1) and (1, 0)\n",
    "            return self._reward_mult if action == True else 0\n",
    "        else:  # (0, 0) and (1, 1)\n",
    "            return -self._punish_mult if action == True else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rate Coding\n",
    "\n",
    "The goal of this experiment is to train a spiking neural network to mimic a XOR gate, meaning it will take two binary inputs and return one binary output. The desired input output mapping is as follows,\n",
    "```\n",
    "0, 0 -> 0\n",
    "0, 1 -> 1\n",
    "1, 0 -> 1\n",
    "1, 1 -> 0\n",
    "```\n",
    "In this specific experiment the inputs are rate coded. There are two input groups, corresponding to the two boolean inputs. If the specific input is 0, its group will not fire at all, otherwise it will at a rate of 40hz. There should be 60 input neurons(30 per group), 60 hidden and 1 output neuron with each layer fully connected to the next. Each input pattern would be presented to the network for 500ms, with 800 patterns being shown in total. While a pattern is being shown, if the correct output is 1, whenever the network's output neuron fires it recieves a reward of 1. Otherwise when the network's output fires it recieves a reward of -1.\n",
    "\n",
    "Converting this description for use in the framework is straightforward, but if it's your first time(s) needs a frame of reference.\n",
    "\n",
    "1. Divide experiment into network and game mechanics.\n",
    "Splitting the mechanics of the network and game here are simple, the game simply gives two random boolean inputs at every timestep and the network respond to these inputs.\n",
    "\n",
    "2. Set up network inputs.\n",
    "We use the RateMap input type for this experiment. The main parameter of this input type is 'state_rate_map' for which we construct a dictionary as follows,\n",
    "```\n",
    "LOW_RATE = 0\n",
    "HIGH_RATE = frequency / steps per pattern = 40 / 500\n",
    "state_rate_map = {\n",
    "    (0, 0): [LOW_RATE, LOW_RATE],\n",
    "    (0, 1): [LOW_RATE, HIGH_RATE],\n",
    "    (1, 0): [HIGH_RATE, LOW_RATE],\n",
    "    (1, 1): [HIGH_RATE, HIGH_RATE],\n",
    "}\n",
    "```\n",
    "The RateMap will do the work to split the input neurons into two groups, each with a respective rate given by state_rate_map\\[current_state].\n",
    "\n",
    "3. Set topology of network.\n",
    "For this we use a manually configured network. We give it one matrix for each layer: the input-hidden and hidden-output layers.\n",
    "Each initial weight value is sampled uniformly between 0 and .2, with these parameters chosen by trial and error.\n",
    "```\n",
    "w_matrix = [\n",
    "    np.random.uniform(0, .2, (N_INPUTS, N_HIDDEN)),\n",
    "    np.random.uniform(0, .2, (N_HIDDEN, N_OUTPUTS)),\n",
    "]\n",
    "```\n",
    "\n",
    "4. Set reward scheme and network readout.\n",
    "This is typically the most complex part of constructing an experiment with spikey, and this example is no different.\n",
    "Here we use the ActiveRLNetwork base so that our reward function is called at every network update, whereas RLNetwork calls the reward every game step, or every PROCESSING_TIME network steps.\n",
    "In conjunction we use the parameter continuous_rwd_action in place of the Readout part since the Readout is only meant to apply at every game step. continuous_rwd_action will tell the rewarder whether or not the output neuron fired via the action paramterer.\n",
    "Finally we use the custom Florian rewarder(defined in the cell above) that functions exactly as the original experiment states. If the expected action is one, a reward is given on every output spike. Otherwise a punishment is given on every output spike.\n",
    "\n",
    "5. Set other parameters.\n",
    "Most of the other parameters are taken directly from the paper or are intuitively chosen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'n_episodes': 1,\n",
    "    'len_episode': 800,\n",
    "    'eval_steps': 50, \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INPUTS = 60\n",
    "N_NEURONS = 61\n",
    "N_OUTPUTS = 1\n",
    "N_HIDDEN = N_NEURONS - N_OUTPUTS\n",
    "PROCESSING_TIME = 500\n",
    "\n",
    "w_matrix = [\n",
    "    np.random.uniform(0, .2, (N_INPUTS, N_HIDDEN)),\n",
    "    np.random.uniform(0, .2, (N_HIDDEN, N_OUTPUTS)),\n",
    "]\n",
    "\n",
    "LOW_RATE = 0\n",
    "HIGH_RATE = 40 / PROCESSING_TIME\n",
    "state_rate_map = {# 2 input groups. 0hz when group false, 40hz when true\n",
    "    (0, 0): np.array([LOW_RATE, LOW_RATE]),\n",
    "    (0, 1): np.array([LOW_RATE, HIGH_RATE]),\n",
    "    (1, 0): np.array([HIGH_RATE, LOW_RATE]),\n",
    "    (1, 1): np.array([HIGH_RATE, HIGH_RATE]),\n",
    "}\n",
    "\n",
    "class network_template(ActiveRLNetwork):\n",
    "    parts = {\n",
    "        'inputs': input.RateMap,\n",
    "        'neurons': neuron.Neuron,\n",
    "        'synapses': synapse.RLSTDP,\n",
    "        'weights': weight.Manual,\n",
    "        'readout': readout.Threshold,\n",
    "        'rewarder': FlorianReward,\n",
    "    }\n",
    "    keys = {\n",
    "        \"n_inputs\": N_INPUTS,\n",
    "        'n_neurons': N_NEURONS,\n",
    "        \"n_outputs\": N_OUTPUTS,\n",
    "        'matrix': w_matrix,\n",
    "\n",
    "        'input_pct_inhibitory': .5,\n",
    "        'neuron_pct_inhibitory': 0,\n",
    "        'magnitude': 1,\n",
    "        'firing_threshold': 16,\n",
    "        'refractory_period': 0,  # Gutig, Aharonov, Rotter, & Sompolinsky 2003\n",
    "        'prob_rand_fire': .15,\n",
    "        'potential_decay': .05,  # Decay constant Tau=20ms, lambda=e^(-t/T)\n",
    "        'trace_decay': .04,  # T_z = 25, lambda = e^(-1/T_z)\n",
    "        \"punish_mult\": 1,\n",
    "\n",
    "        'processing_time': PROCESSING_TIME,\n",
    "        'learning_rate': .625 / 25,  # gamma_0 = gamma / Tau_z\n",
    "        'max_weight': 5,\n",
    "        'stdp_window': 20,  # Tau_+ = Tau_- = 20ms\n",
    "        'action_threshold': 0,  # Makes network always output True\n",
    "\n",
    "        'continuous_rwd_action': lambda network, game: network.spike_log[-1, -1],\n",
    "        'state_rate_map': state_rate_map,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26469)\u001b[0m 2022-12-11 11:59:42,686\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26469)\u001b[0m (False, False): 0.00 -> 0.15\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26469)\u001b[0m (False, True): 0.04 -> 0.14\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26469)\u001b[0m (True, False): 0.04 -> 0.15\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26469)\u001b[0m (True, True): 0.08 -> 0.15\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26469)\u001b[0m 362 -> 362\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26469)\u001b[0m 6 -> 6\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26469)\u001b[0m 0.1471 0.1555\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26469)\u001b[0m Florian - Win: False, Accuracy: 0.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 12:00:49,981\tERROR checkpoint_manager.py:327 -- Result dict has no key: training_iteration. checkpoint_score_attr must be set to a key in the result dict. Valid keys are: ['trial_id', 'experiment_id', 'date', 'timestamp', 'pid', 'hostname', 'node_ip', 'done']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial TorchTrainer_580c3_00000 completed. Last result: \n"
     ]
    }
   ],
   "source": [
    "# Control, without learning\n",
    "def train_func():\n",
    "    network_template.keys.update({'learning_rate': 0, 'len_episode': 50})\n",
    "    game = Logic(preset=\"XOR\")\n",
    "    model = network_template()\n",
    "\n",
    "    original_w = model.synapses.weights.matrix.copy()\n",
    "    inrates = []\n",
    "    sysrates = []\n",
    "    outrates = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    for epoch in range(5):\n",
    "        model.reset()\n",
    "        state = game.reset()\n",
    "        state_next = None\n",
    "\n",
    "        inrates.append([])\n",
    "        sysrates.append([])\n",
    "        outrates.append([])\n",
    "        states.append([])\n",
    "        actions.append([])\n",
    "        for s in range(100):\n",
    "            action = model.tick(state)\n",
    "            state_next, _, done, __ = game.step(action)\n",
    "            reward = model.reward(state, action, state_next)\n",
    "\n",
    "            inrates[-1].append(np.mean(np.abs(model.spike_log[-model._processing_time:, :model._n_inputs])))\n",
    "            sysrates[-1].append(np.mean(np.abs(model.spike_log[-model._processing_time:, model._n_inputs:-model._n_outputs])))\n",
    "            outrates[-1].append(np.mean(np.abs(model.spike_log[-model._processing_time:, -model._n_outputs:])))\n",
    "            states[-1].append(state)\n",
    "            actions[-1].append(action)\n",
    "\n",
    "            state = state_next\n",
    "            if done:\n",
    "                break\n",
    "    print_rates(step_inrates=inrates, step_outrates=outrates, step_states=states, observation_space=game.observation_space)\n",
    "    print_w_diffs(original_w, model.synapses.weights.matrix, model._n_inputs)\n",
    "    print_success(states, inrates, sysrates, outrates, training_params)\n",
    "\n",
    "    return {}\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=ScalingConfig(num_workers=1),\n",
    "    run_config=RunConfig(verbose=0),\n",
    ")\n",
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26651)\u001b[0m 2022-12-11 12:00:54,470\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26651)\u001b[0m (False, False): 0.00 -> 0.69\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26651)\u001b[0m (False, True): 0.04 -> 0.86\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26651)\u001b[0m (True, False): 0.04 -> 0.90\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26651)\u001b[0m (True, True): 0.08 -> 0.69\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26651)\u001b[0m 362 -> 12924\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26651)\u001b[0m 6 -> 116\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26651)\u001b[0m 0.8267 0.7085\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26651)\u001b[0m Florian - Win: True, Accuracy: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 12:02:05,413\tERROR checkpoint_manager.py:327 -- Result dict has no key: training_iteration. checkpoint_score_attr must be set to a key in the result dict. Valid keys are: ['trial_id', 'experiment_id', 'date', 'timestamp', 'pid', 'hostname', 'node_ip', 'done']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial TorchTrainer_82998_00000 completed. Last result: \n"
     ]
    }
   ],
   "source": [
    "# Real experiment\n",
    "def train_func():\n",
    "    game = Logic(preset=\"XOR\")\n",
    "    model = network_template()\n",
    "\n",
    "    original_w = model.synapses.weights.matrix.copy()\n",
    "    inrates = []\n",
    "    sysrates = []\n",
    "    outrates = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    for epoch in range(5):\n",
    "        model.reset()\n",
    "        state = game.reset()\n",
    "        state_next = None\n",
    "\n",
    "        inrates.append([])\n",
    "        sysrates.append([])\n",
    "        outrates.append([])\n",
    "        states.append([])\n",
    "        actions.append([])\n",
    "        for s in range(100):\n",
    "            action = model.tick(state)\n",
    "            state_next, _, done, __ = game.step(action)\n",
    "            reward = model.reward(state, action, state_next)\n",
    "\n",
    "            inrates[-1].append(np.mean(np.abs(model.spike_log[-model._processing_time:, :model._n_inputs])))\n",
    "            sysrates[-1].append(np.mean(np.abs(model.spike_log[-model._processing_time:, model._n_inputs:-model._n_outputs])))\n",
    "            outrates[-1].append(np.mean(np.abs(model.spike_log[-model._processing_time:, -model._n_outputs:])))\n",
    "            states[-1].append(state)\n",
    "            actions[-1].append(action)\n",
    "\n",
    "            state = state_next\n",
    "            if done:\n",
    "                break\n",
    "    print_rates(step_inrates=inrates, step_outrates=outrates, step_states=states, observation_space=game.observation_space)\n",
    "    print_w_diffs(original_w, model.synapses.weights.matrix, model._n_inputs)\n",
    "    print_success(states, inrates, sysrates, outrates, training_params)\n",
    "\n",
    "    return {}\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=ScalingConfig(num_workers=1),\n",
    "    run_config=RunConfig(verbose=0),\n",
    ")\n",
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Coding\n",
    "\n",
    "The goal of this experiment is to train a spiking neural network to mimic a XOR gate, meaning it will take two binary inputs and return one binary output. The desired input output mapping is as follows,\n",
    "```\n",
    "0, 0 -> 0\n",
    "0, 1 -> 1\n",
    "1, 0 -> 1\n",
    "1, 1 -> 0\n",
    "```\n",
    "In this specific experiment the inputs are temporal coded. There are two input groups, corresponding to the two boolean inputs. Each input value, 0 or 1, has a static spike train that is shared between input groups and is triggered whenever that input value is used. There should be 60 input neurons(30 per group), 60 hidden and 1 output neuron with each layer fully connected to the next. Each input pattern would be presented to the network for 500ms, with 800 patterns being shown in total. While a pattern is being shown, if the correct output is 1, whenever the network's output neuron fires it recieves a reward of 1. Otherwise when the network's output fires it recieves a reward of -1.\n",
    "\n",
    "Converting this description for use in the framework is straightforward, but if it's your first time(s) needs a frame of reference.\n",
    "\n",
    "1. Divide experiment into network and game mechanics.\n",
    "Splitting the mechanics of the network and game here are simple, the game simply gives two random boolean inputs at every timestep and the network respond to these inputs.\n",
    "\n",
    "2. Set up network inputs.\n",
    "Here we used temporally coded inputs, each input value, 0 or 1, corresponds to a static spike train that is shared between input groups. Each time the input value is given the spike train is shown by all neurons in the input group.\n",
    "We accomplish this using the StaticMap input type. This works similarly to the RateMap used before, in that we give it an input value to spike train mapping. This can be done on a per input or an aggregate basis, here we do in aggregate for readability.\n",
    "```\n",
    "LOW_TRAIN = np.int_(np.random.uniform(0, 1, (PROCESSING_TIME, N_INPUTS // 2)) <= 50 * .0001)\n",
    "HIGH_TRAIN = np.int_(np.random.uniform(0, 1, (PROCESSING_TIME, N_INPUTS // 2)) <= 50 * .0001)\n",
    "input_map = {\n",
    "    (0, 0): np.hstack((LOW_TRAIN, LOW_TRAIN)),\n",
    "    (0, 1): np.hstack((LOW_TRAIN, HIGH_TRAIN)),\n",
    "    (1, 0): np.hstack((HIGH_TRAIN, LOW_TRAIN)),\n",
    "    (1, 1): np.hstack((HIGH_TRAIN, HIGH_TRAIN)),\n",
    "}\n",
    "```\n",
    "\n",
    "3. Set topology of network.\n",
    "For this we use a manually configured network. We give it one matrix for each layer: the input-hidden and hidden-output layers.\n",
    "Each initial weight value is sampled uniformly between 0 and .2, with these parameters chosen by trial and error.\n",
    "```\n",
    "w_matrix = [\n",
    "    np.random.uniform(0, .2, (N_INPUTS, N_HIDDEN)),\n",
    "    np.random.uniform(0, .2, (N_HIDDEN, N_OUTPUTS)),\n",
    "]\n",
    "```\n",
    "\n",
    "4. Set reward scheme and network readout.\n",
    "This is typically the most complex part of constructing an experiment with spikey, and this example is no different.\n",
    "Here we use the ActiveRLNetwork base so that our reward function is called at every network update, whereas RLNetwork calls the reward every game step, or every PROCESSING_TIME network steps.\n",
    "In conjunction we use the parameter continuous_rwd_action in place of the Readout part since the Readout is only meant to apply at every game step. continuous_rwd_action will tell the rewarder whether or not the output neuron fired via the action paramterer.\n",
    "Finally we use the custom Florian rewarder(defined in the cell above) that functions exactly as the original experiment states. If the expected action is one, a reward is given on every output spike. Otherwise a punishment is given on every output spike.\n",
    "\n",
    "5. Set other parameters.\n",
    "Most of the other parameters are taken directly from the paper or are intuitively chosen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    'n_episodes': 1,\n",
    "    'len_episode': 800,\n",
    "    'eval_steps': 50,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_INPUTS = 2\n",
    "N_NEURONS = 21\n",
    "N_OUTPUTS = 1\n",
    "N_HIDDEN = N_NEURONS - N_OUTPUTS\n",
    "PROCESSING_TIME = 500\n",
    "\n",
    "LOW_TRAIN = np.int_(np.random.uniform(0, 1, (PROCESSING_TIME, N_INPUTS // 2)) <= 50 * .0001)\n",
    "HIGH_TRAIN = np.int_(np.random.uniform(0, 1, (PROCESSING_TIME, N_INPUTS // 2)) <= 50 * .0001)\n",
    "input_map = {\n",
    "    (0, 0): np.hstack((LOW_TRAIN, LOW_TRAIN)),\n",
    "    (0, 1): np.hstack((LOW_TRAIN, HIGH_TRAIN)),\n",
    "    (1, 0): np.hstack((HIGH_TRAIN, LOW_TRAIN)),\n",
    "    (1, 1): np.hstack((HIGH_TRAIN, HIGH_TRAIN)),\n",
    "}\n",
    "\n",
    "w_matrix = [\n",
    "    np.random.uniform(0, .4, (N_INPUTS, N_HIDDEN)),\n",
    "    np.random.uniform(0, .4, (N_HIDDEN, N_OUTPUTS)),\n",
    "]\n",
    "class network_template(ActiveRLNetwork):\n",
    "    parts = {\n",
    "        'inputs': input.StaticMap,\n",
    "        'neurons': neuron.Neuron,\n",
    "        'synapses': synapse.RLSTDP,\n",
    "        'weights': weight.Manual,\n",
    "        'readout': readout.Threshold,\n",
    "        'rewarder': FlorianReward,\n",
    "    }\n",
    "    keys = {\n",
    "        \"n_inputs\": N_INPUTS,\n",
    "        'n_neurons': N_NEURONS,\n",
    "        \"n_outputs\": N_OUTPUTS,\n",
    "        'matrix': w_matrix,\n",
    "\n",
    "        'input_pct_inhibitory': .5,\n",
    "        'neuron_pct_inhibitory': 0,\n",
    "        'magnitude': 1,\n",
    "        'firing_threshold': 16,\n",
    "        'refractory_period': 0,  # Gutig, Aharonov, Rotter, & Sompolinsky 2003\n",
    "        'prob_rand_fire': .15,\n",
    "        'potential_decay': .05,  # Decay constant Tau=20ms, lambda=e^(-t/T)\n",
    "        'trace_decay': .04,  # T_z = 25, lambda = e^(-1/T_z)\n",
    "        \"punish_mult\": 1,\n",
    "\n",
    "        'processing_time': PROCESSING_TIME,\n",
    "        'learning_rate': .5 / 25,  # gamma_0 = gamma / Tau_z\n",
    "        'max_weight': 5,\n",
    "        'stdp_window': 20,  # Tau_+ = Tau_- = 20ms\n",
    "        'action_threshold': 0,  # Makes network always output True\n",
    "\n",
    "        'continuous_rwd_action': lambda network, game: network.spike_log[-1, -1],\n",
    "        'state_spike_map': input_map,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26761)\u001b[0m 2022-12-11 12:02:09,982\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26761)\u001b[0m (False, False): 0.00 -> 0.15\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26761)\u001b[0m (False, True): 0.01 -> 0.15\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26761)\u001b[0m (True, False): 0.01 -> 0.15\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26761)\u001b[0m (True, True): 0.01 -> 0.15\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26761)\u001b[0m 1 -> 1\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26761)\u001b[0m 11 -> 11\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26761)\u001b[0m 0.1495 0.1595\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26761)\u001b[0m Florian - Win: False, Accuracy: 0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 12:03:18,863\tERROR checkpoint_manager.py:327 -- Result dict has no key: training_iteration. checkpoint_score_attr must be set to a key in the result dict. Valid keys are: ['trial_id', 'experiment_id', 'date', 'timestamp', 'pid', 'hostname', 'node_ip', 'done']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial TorchTrainer_afa98_00000 completed. Last result: \n"
     ]
    }
   ],
   "source": [
    "# Control, without learning\n",
    "def train_func():\n",
    "    network_template.keys.update({'learning_rate': 0, 'len_episode': 50})\n",
    "    game = Logic(preset=\"XOR\")\n",
    "    model = network_template()\n",
    "\n",
    "    original_w = model.synapses.weights.matrix.copy()\n",
    "    inrates = []\n",
    "    sysrates = []\n",
    "    outrates = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    for epoch in range(5):\n",
    "        model.reset()\n",
    "        state = game.reset()\n",
    "        state_next = None\n",
    "\n",
    "        inrates.append([])\n",
    "        sysrates.append([])\n",
    "        outrates.append([])\n",
    "        states.append([])\n",
    "        actions.append([])\n",
    "        for s in range(100):\n",
    "            action = model.tick(state)\n",
    "            state_next, _, done, __ = game.step(action)\n",
    "            reward = model.reward(state, action, state_next)\n",
    "\n",
    "            inrates[-1].append(np.mean(np.abs(model.spike_log[-model._processing_time:, :model._n_inputs])))\n",
    "            sysrates[-1].append(np.mean(np.abs(model.spike_log[-model._processing_time:, model._n_inputs:-model._n_outputs])))\n",
    "            outrates[-1].append(np.mean(np.abs(model.spike_log[-model._processing_time:, -model._n_outputs:])))\n",
    "            states[-1].append(state)\n",
    "            actions[-1].append(action)\n",
    "\n",
    "            state = state_next\n",
    "            if done:\n",
    "                break\n",
    "    print_rates(step_inrates=inrates, step_outrates=outrates, step_states=states, observation_space=game.observation_space)\n",
    "    print_w_diffs(original_w, model.synapses.weights.matrix, model._n_inputs)\n",
    "    print_success(states, inrates, sysrates, outrates, training_params)\n",
    "\n",
    "    return {}\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=ScalingConfig(num_workers=1),\n",
    "    run_config=RunConfig(verbose=0),\n",
    ")\n",
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26870)\u001b[0m 2022-12-11 12:03:23,601\tINFO config.py:87 -- Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26870)\u001b[0m (False, False): 0.00 -> 0.35\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26870)\u001b[0m (False, True): 0.01 -> 0.39\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26870)\u001b[0m (True, False): 0.01 -> 0.40\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26870)\u001b[0m (True, True): 0.01 -> 0.34\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26870)\u001b[0m 1 -> 6\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26870)\u001b[0m 11 -> 104\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26870)\u001b[0m 0.416 0.3682\n",
      "\u001b[2m\u001b[36m(RayTrainWorker pid=26870)\u001b[0m Florian - Win: True, Accuracy: 0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-11 12:04:29,959\tERROR checkpoint_manager.py:327 -- Result dict has no key: training_iteration. checkpoint_score_attr must be set to a key in the result dict. Valid keys are: ['trial_id', 'experiment_id', 'date', 'timestamp', 'pid', 'hostname', 'node_ip', 'done']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial TorchTrainer_db511_00000 completed. Last result: \n"
     ]
    }
   ],
   "source": [
    "# Real experiment\n",
    "def train_func():\n",
    "    game = Logic(preset=\"XOR\")\n",
    "    model = network_template()\n",
    "\n",
    "    original_w = model.synapses.weights.matrix.copy()\n",
    "    inrates = []\n",
    "    sysrates = []\n",
    "    outrates = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    for epoch in range(5):\n",
    "        model.reset()\n",
    "        state = game.reset()\n",
    "        state_next = None\n",
    "\n",
    "        inrates.append([])\n",
    "        sysrates.append([])\n",
    "        outrates.append([])\n",
    "        states.append([])\n",
    "        actions.append([])\n",
    "        for s in range(100):\n",
    "            action = model.tick(state)\n",
    "            state_next, _, done, __ = game.step(action)\n",
    "            reward = model.reward(state, action, state_next)\n",
    "\n",
    "            inrates[-1].append(np.mean(np.abs(model.spike_log[-model._processing_time:, :model._n_inputs])))\n",
    "            sysrates[-1].append(np.mean(np.abs(model.spike_log[-model._processing_time:, model._n_inputs:-model._n_outputs])))\n",
    "            outrates[-1].append(np.mean(np.abs(model.spike_log[-model._processing_time:, -model._n_outputs:])))\n",
    "            states[-1].append(state)\n",
    "            actions[-1].append(action)\n",
    "\n",
    "            state = state_next\n",
    "            if done:\n",
    "                break\n",
    "    print_rates(step_inrates=inrates, step_outrates=outrates, step_states=states, observation_space=game.observation_space)\n",
    "    print_w_diffs(original_w, model.synapses.weights.matrix, model._n_inputs)\n",
    "    print_success(states, inrates, sysrates, outrates, training_params)\n",
    "\n",
    "    return {}\n",
    "\n",
    "trainer = TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=ScalingConfig(num_workers=1),\n",
    "    run_config=RunConfig(verbose=0),\n",
    ")\n",
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "c671e72284273e5be387b60af2ce98411d2fa29290b07b37da2690b81960f96b"
   }
  },
  "orig_nbformat": 2,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
