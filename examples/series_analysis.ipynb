{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12/31 No Layer Learning OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from spikey.logging import Reader\n",
    "from spikey.viz import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = os.path.join('..', 'log', 'no_layer')\n",
    "FOLDERS = os.listdir(base)\n",
    "\n",
    "dataset = {folder: Reader(folder=os.path.join(base, folder)) for folder in FOLDERS if os.path.isdir(os.path.join(base, folder))}\n",
    "del FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_KEYS = ['weight', 'input', 'neuron', 'synapse', 'readout', 'firing_threshold',\n",
    "       'processing_time', 'magnitude', 'potential_decay', 'n_neurons',\n",
    "       'neuron_pct_inhibitory', 'prob_rand_fire', 'refractory_period', 'resting_mv',\n",
    "       'spike_delay', 'stdp_window', 'learning_rate', 'max_weight', 'trace_decay',\n",
    "       'n_episodes', 'len_episode', 'n_inputs', # 'step_size', 'connection_probability', \n",
    "       'n_outputs', 'name', 'reward_mult', 'punish_mult', 'prob_rand_fire_setter']\n",
    "HASHABLE_CONFIG_KEYS = ['weight', 'input', 'neuron', 'synapse', 'readout', 'firing_threshold',\n",
    "       'processing_time', 'magnitude', 'potential_decay', 'n_neurons',\n",
    "       'neuron_pct_inhibitory', 'prob_rand_fire', 'refractory_period', 'resting_mv',\n",
    "       'spike_delay', 'stdp_window', 'learning_rate', 'max_weight', 'trace_decay',\n",
    "       'n_episodes', 'len_episode', 'n_inputs', # 'step_size', 'connection_probability', \n",
    "       'n_outputs', 'name', 'reward_mult', 'punish_mult']\n",
    "RESULT_KEYS = ['cluster_coeff_original', 'cluster_coeff_final', 'accuracy', 'action_entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average Results for Params\n",
    "mean_dataset = {}\n",
    "\n",
    "for label, reader in dataset.items():\n",
    "    data = pd.DataFrame(columns=CONFIG_KEYS + RESULT_KEYS)\n",
    "\n",
    "    key = label.split('-')[-1]\n",
    "\n",
    "    for i, relevant in enumerate(reader.iter_unique(CONFIG_KEYS + RESULT_KEYS, HASHABLE_CONFIG_KEYS)):        \n",
    "        temp = {}\n",
    "        for key in RESULT_KEYS:\n",
    "            temp[key] = np.mean(relevant[key])\n",
    "\n",
    "        data.loc[i] = list(relevant[CONFIG_KEYS].iloc[0]) + [temp[key] for key in RESULT_KEYS]\n",
    "\n",
    "    mean_dataset[label] = data\n",
    "\n",
    "del data, relevant, temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mean_dataset.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Output rate basins of attraction\n",
    "for label, reader in dataset.items():\n",
    "    out_basins(reader['step_inrates'], reader['step_outrates'], title=label)\n",
    "del reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 'accuracy'\n",
    "y = 'action_entropy'\n",
    "\n",
    "for label, reader in dataset.items():\n",
    "    X = reader.df[x]\n",
    "    Y = reader.df[y]\n",
    "\n",
    "    plt.scatter(X, Y)\n",
    "    plt.xlabel(x)\n",
    "    plt.ylabel(y)\n",
    "\n",
    "    X, Y = np.float32(X), np.float32(Y)\n",
    "    plt.plot(np.unique(X), np.poly1d(np.polyfit(X, Y, 1))(np.unique(X)))\n",
    "\n",
    "    plt.title(label)\n",
    "    plt.show()\n",
    "\n",
    "del X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Histogram of Accuracies & Entropies\n",
    "def dfdist(df: DataFrame, value: str, title: str = \"\"):\n",
    "    SCALAR = 1  # To normalize the density to 0, 1\n",
    "\n",
    "    data = df[value]\n",
    "    mean = np.mean(data)\n",
    "\n",
    "    sns.distplot(\n",
    "        data * SCALAR, 10, hist_kws={\"range\": (0, SCALAR)}, kde=False, norm_hist=True\n",
    "    )\n",
    "    plt.plot([mean * SCALAR, mean * SCALAR], [0, 0.2], label=\"mean\", c=\"black\")\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(value)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for label in dataset:\n",
    "    for X in ['accuracy', 'action_entropy']:\n",
    "        dfdist(dataset[label].df, X, title=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in dataset:\n",
    "    zero_entropy = sum(dataset[label]['action_entropy'] == 0) / len(dataset[label]['action_entropy'])\n",
    "    nonzero_entropy = sum(dataset[label]['action_entropy'] != 0) / len(dataset[label]['action_entropy'])\n",
    "\n",
    "    print(f\"{label}: Entropies: Zero {zero_entropy}%, Nonzero {nonzero_entropy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in dataset:\n",
    "    maxx = np.max(dataset[label]['accuracy'])\n",
    "    minn = np.min(dataset[label]['accuracy'])\n",
    "\n",
    "    print(f\"{label}: Max Accuracy {maxx}%, min Accuracy {minn}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Quartiles Table\n",
    "columns = ['Label', '25% accuracy', '50% accuracy', '75% accuracy', 'std accuracy', '25% entropy', '50% entropy', '75% entropy', 'std entropy']\n",
    "quartiles = pd.DataFrame(columns=columns)\n",
    "\n",
    "for i, (label, df) in enumerate(dataset.items()):\n",
    "    df = df.df\n",
    "    accuracy = df['accuracy']\n",
    "    entropy = df['action_entropy']\n",
    "\n",
    "    quartile1_accuracy, median_accuracy, quartile3_accuracy = accuracy.quantile([.25, .5, .75])\n",
    "    quartile1_entropy, median_entropy, quartile3_entropy = entropy.quantile([.25, .5, .75])\n",
    "    std_accuracy, std_entropy = np.std(accuracy), np.std(entropy)\n",
    "\n",
    "    output = {\n",
    "        'Label': label, \n",
    "        '25% accuracy': quartile1_accuracy,\n",
    "        '50% accuracy': median_accuracy, \n",
    "        '75% accuracy': quartile3_accuracy,\n",
    "        'std accuracy': std_accuracy,\n",
    "        '25% entropy': quartile1_entropy,\n",
    "        '50% entropy': median_entropy, \n",
    "        '75% entropy': quartile3_entropy,\n",
    "        'std entropy': std_entropy,\n",
    "    }\n",
    "    quartiles.loc[i] = list(output.values())\n",
    "\n",
    "## results.to_csv('results.csv')\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(quartiles.to_html()))\n",
    "\n",
    "del quartiles, output, df, accuracy, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Shotgun of Effects of Param Changes w/out scatter\n",
    "for label, df in dataset.items():\n",
    "    if label in ['reward', 'matrix']:\n",
    "        continue\n",
    "    df = df.df\n",
    "\n",
    "    X = label.split('-')[-1] if label != 'control' else 'weight'\n",
    "    Y1 = 'accuracy'\n",
    "    Y2 = 'action_entropy'\n",
    "\n",
    "    x = df[X]\n",
    "    y1 = df[Y1]\n",
    "    y2 = df[Y2]\n",
    "\n",
    "    mean_x = mean_dataset[label][X]\n",
    "    mean_y1 = mean_dataset[label][Y1]\n",
    "    mean_y2 = mean_dataset[label][Y2]\n",
    "\n",
    "    if isinstance(x[0], np.ndarray):\n",
    "        x = [np.mean(value) for value in x.values]\n",
    "        mean_x = [np.mean(value) for value in mean_x.values]\n",
    "\n",
    "    plt.plot(mean_x, mean_y1, label=Y1)\n",
    "\n",
    "    plt.plot(mean_x, mean_y2, label=Y2)\n",
    "\n",
    "    plt.xlabel(label)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "del y1, y2, mean_y1, mean_y2, x, mean_x, X, Y1, Y2, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy violin plots\n",
    "\n",
    "for label, df in dataset.items():\n",
    "    df = df.df\n",
    "\n",
    "    X = label.split('-')[-1] if label != 'control' else 'weight'\n",
    "    Y1 = 'accuracy'\n",
    "\n",
    "    x = df[X]\n",
    "    y1 = df[Y1]\n",
    "\n",
    "    if isinstance(x[0], np.ndarray):\n",
    "        x = [np.mean(value) for value in x.values]\n",
    "\n",
    "    sns.violinplot(x=x, y=y1)\n",
    "\n",
    "    plt.title(label)\n",
    "    plt.show()\n",
    "\n",
    "del x, y1, X, Y1, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ax = sns.violinplot(x=\"day\", y=\"total_bill\", hue=\"smoker\",\n",
    "#                     data=tips, palette=\"muted\")\n",
    "\n",
    "for label, df in dataset.items():\n",
    "    df = df.df\n",
    "\n",
    "    if 'control' in label:\n",
    "        continue\n",
    "\n",
    "    X = label.split('-')[-1]\n",
    "    Y1 = 'accuracy'\n",
    "    Y2 = 'action_entropy'\n",
    "\n",
    "    x = df[X]\n",
    "    y1 = df[Y1]\n",
    "    y2 = df[Y2]\n",
    "\n",
    "    if isinstance(x[0], np.ndarray):\n",
    "        x = [np.mean(value) for value in x.values]\n",
    "\n",
    "    sns.violinplot(x=np.append(x, 0), y=np.append(y1, 0), hue=np.append(np.zeros(len(x)), [1]), split=True)\n",
    "    sns.violinplot(x=np.append(x, 0), y=np.append(y2, 0), hue=np.append(np.ones(len(x)), [0]), split=True)\n",
    "\n",
    "    plt.title(label)\n",
    "    plt.show()\n",
    "\n",
    "    del x, y1, y2, X, Y1, Y2, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Effect of Label on Accuracy & Entropy\n",
    "for label, df in mean_dataset.items():\n",
    "    try:\n",
    "        if 'control' in label:\n",
    "            display(HTML(df.to_html()))\n",
    "        else:\n",
    "            display(HTML(df[[label.split('-')[-1], 'accuracy', 'action_entropy']].to_html()))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "del df, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate average confusion matricies per config setting\n",
    "for label, reader in dataset.items():\n",
    "    if label in ['reward', 'matrix']:\n",
    "        continue\n",
    "    key = label.split('-')[-1]\n",
    "\n",
    "    for value, confusions in reader.iter_unique('confusion_final', HASHABLE_CONFIG_KEYS, return_value=True):\n",
    "        #confusions = reader['confusion_final', np.where((reader.df[HASHABLE_CONFIG_KEYS] == value[1]).all(axis='columns'))[0]]\n",
    "        # [expected][real]\n",
    "        O_STATES = [\"0\", \"1\"]\n",
    "        output = {e: {r: 0 for r in O_STATES} for e in [\"True\", \"False\"]}\n",
    "\n",
    "        n_states = 0\n",
    "        e_counts = {key: 0 for key in [\"True\", \"False\"]}\n",
    "\n",
    "        ## Sum\n",
    "        for e_state in [\"True\", \"False\"]:\n",
    "            for r_state in O_STATES:\n",
    "                for confusion in confusions:\n",
    "                    if e_state not in confusion or r_state not in confusion[e_state]:\n",
    "                        continue\n",
    "                    \n",
    "                    count = int(confusion[e_state][r_state])\n",
    "\n",
    "                    output[e_state][r_state] += count\n",
    "                    e_counts[e_state] += count\n",
    "                    n_states += count\n",
    "\n",
    "        ## Average\n",
    "        for e_state in [\"True\", \"False\"]:\n",
    "            for r_state in O_STATES:\n",
    "                if not e_counts[e_state]:\n",
    "                    continue\n",
    "\n",
    "                output[e_state][r_state] = round(output[e_state][r_state] / e_counts[e_state], 2)\n",
    "\n",
    "        print(f\"{label} {value[1][key] if key != 'control' else 'control':.2} {n_states}\")\n",
    "        #print(f\"{key} {n_states}\")\n",
    "        \n",
    "        for e_state, r_states in output.items():\n",
    "            print(f\"{e_state}: {r_states}\")\n",
    "        print()\n",
    "del output, confusions, reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scatter accuracy of expected 1 vs expected 0\n",
    "for label, reader in dataset.items():\n",
    "    if label in ['reward', 'matrix']:\n",
    "        continue\n",
    "    unique_params = reader.df[HASHABLE_CONFIG_KEYS].drop_duplicates(inplace=False)\n",
    "\n",
    "    key = label.split('-')[-1]\n",
    "\n",
    "    for value, confusions in reader.iter_unique('confusion_final', HASHABLE_CONFIG_KEYS, return_value=True):\n",
    "        O_STATES = [\"True\", \"False\"]\n",
    "        O_STATES2 = [\"0\", \"1\"]\n",
    "\n",
    "        output = {o: [] for o in O_STATES}    \n",
    "\n",
    "        for confusion in confusions:\n",
    "            for e_state, r_states in confusion.items():\n",
    "                n = 0\n",
    "                n_right = 0\n",
    "                for r_state in r_states:\n",
    "                    if e_state not in confusion or r_state not in confusion[e_state]:\n",
    "                        continue\n",
    "\n",
    "                    count = int(confusion[e_state][r_state])\n",
    "\n",
    "                    n += count\n",
    "                    if (e_state == 'True' and r_state  == '1') or (e_state == 'False' and r_state == '0'):\n",
    "                        n_right += count\n",
    "                if n == 0:\n",
    "                    print(confusion, r_states)\n",
    "                output[e_state].append(n_right / n)\n",
    "\n",
    "        ##\n",
    "        o1 = list(output.keys())[0]\n",
    "        o2 = list(output.keys())[1]\n",
    "\n",
    "        output[o1] = [value * 10 for value in output[o1]]\n",
    "\n",
    "        plt.scatter(output[o1], output[o2])\n",
    "        output[o2] = [value * 10 for value in output[o2]]\n",
    "\n",
    "\n",
    "        # NOTE: distplot accuracies of o1\n",
    "        sns.distplot(output[o1], 10, hist_kws={'range': (0, 10)}, kde=False, norm_hist=True)\n",
    "\n",
    "        plt.title(f\"{label} @ {value[1][key] if key != 'control' else 'control'} compare accuracies {o1}, {o2}\")\n",
    "        plt.xlabel(f'accuracies {o1}')\n",
    "        plt.ylabel(f'accuracies {o2}')\n",
    "        plt.show()\n",
    "del output, confusions, reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count number of confusion matricies w/ acc(true) + acc(false) > 55%\n",
    "## Scatter accuracy of expected 1 vs expected 0\n",
    "for label, reader in dataset.items():\n",
    "    if label in ['reward', 'matrix']:\n",
    "        continue\n",
    "    unique_params = reader.df[HASHABLE_CONFIG_KEYS].drop_duplicates(inplace=False)\n",
    "\n",
    "    key = label.split('-')[-1]\n",
    "    for value, confusions in reader.iter_unique('confusion_final', HASHABLE_CONFIG_KEYS, return_value=True):\n",
    "\n",
    "        O_STATES = [\"True\", \"False\"]\n",
    "        O_STATES2 = [\"0\", \"1\"]\n",
    "\n",
    "        output = {o: [] for o in O_STATES}    \n",
    "        n_greater = 0\n",
    "        for confusion in confusions:\n",
    "            acc = 0\n",
    "\n",
    "            for e_state, r_states in confusion.items():\n",
    "                n = 0\n",
    "                n_right = 0\n",
    "                for r_state in r_states:\n",
    "                    if e_state not in confusion or r_state not in confusion[e_state]:\n",
    "                        continue\n",
    "\n",
    "                    count = int(confusion[e_state][r_state])\n",
    "\n",
    "                    n += count\n",
    "                    if (e_state == 'True' and r_state  == '1') or (e_state == 'False' and r_state == '0'):\n",
    "                        n_right += count\n",
    "\n",
    "                acc += n_right / n\n",
    "            if acc >= 1.05:\n",
    "                n_greater += 1\n",
    "\n",
    "        print(f\"{label} @ {value[1][key] if key != 'control' else 'control'} n class accuracies > 105% {n_greater} / {len(confusions)}\")\n",
    "del confusions, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dW Bin Counts\n",
    "\"\"\"\n",
    "for key, reader in dataset.items():\n",
    "    final_w = np.ravel(reader['weights_final', 1][0])\n",
    "    original_w = np.ravel(reader['weights_original', 1][0])\n",
    "\n",
    "    if isinstance(final_w[0], str):\n",
    "        print(f\"W did not read properly on {key}!\")\n",
    "        continue\n",
    "\n",
    "    delta_w = final_w - original_w\n",
    "\n",
    "    normalized_delta_w = np.int_(delta_w * 5) + 5\n",
    "\n",
    "    n_converges_zero = np.sum((original_w != 0) & (final_w == 0))\n",
    "\n",
    "    ## Remove all 0->0s from bincount\n",
    "    normalized_delta_w = normalized_delta_w[~((original_w == 0) & (final_w == 0))]\n",
    "\n",
    "    print(key, np.bincount(normalized_delta_w), '\\t\\tconverges to zero', n_converges_zero)\n",
    "del delta_w, normalized_delta_w, n_converges_zero, original_w, final_w\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Average weight change\n",
    "for key, reader in dataset.items():\n",
    "    if key != 'control':\n",
    "        continue\n",
    "\n",
    "    final_w = np.sum(reader['weights_final', 5])\n",
    "    original_w = np.sum(reader['weights_original', 5])     \n",
    "    delta_w = final_w - original\n",
    "    n_nonzero = np.sum(reader['weights_original', 0] != 0)\n",
    "\n",
    "    print(key, delta_w / n_nonzero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot accuracy/entropy changes\n",
    "# Note: this is only for 1 item in dataset!!!\n",
    "\"\"\"\n",
    "for key, reader in dataset.items():\n",
    "    accuracies = reader['ep_action_accuracy', 1][0]\n",
    "    entropies = reader['ep_action_entropy', 1][0]\n",
    "    rewards = reader['ep_rewards', 1][0]\n",
    "\n",
    "    # normalize rewards\n",
    "    try:\n",
    "        rewards = rewards / (reader['len_episode', 1][0] * reader['reward_mult', 1][0])\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "    x = np.arange(len(accuracies))\n",
    "\n",
    "    plt.title(f\"{key}\")\n",
    "    plt.scatter(x, accuracies, label='accuracies')\n",
    "    plt.scatter(x, entropies, label='action_entropies')\n",
    "    plt.scatter(x, rewards, label='normalized ep_rewards')\n",
    "\n",
    "    plt.ylim(-1, 1)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "del rewards, x, entropies, accuracies\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, reader in dataset.items():\n",
    "    if label in ['matrix', 'reward']:\n",
    "        continue\n",
    "    df = reader.df\n",
    "\n",
    "    unique_params = df[HASHABLE_CONFIG_KEYS].drop_duplicates(inplace=False)\n",
    "\n",
    "    key = label.split('-')[-1]\n",
    "    if key == 'control':\n",
    "        key = 'neuron'\n",
    "\n",
    "    ## Todo add double not in df iterator to iter_unique\n",
    "    for i, value in enumerate(unique_params.iterrows()):\n",
    "        cluster_coeff_original = df.loc[np.where((reader.df[HASHABLE_CONFIG_KEYS] == value[1]).all(axis='columns'))]['cluster_coeff_original']\n",
    "        cluster_coeff_final = df.loc[np.where((reader.df[HASHABLE_CONFIG_KEYS] == value[1]).all(axis='columns'))]['cluster_coeff_final']\n",
    "\n",
    "        plt.title(f\"{label} {value[1][key]:.2}\")\n",
    "        plt.scatter(cluster_coeff_original, cluster_coeff_final)\n",
    "        \n",
    "        plt.xlabel('orginal')\n",
    "        plt.ylabel('final')\n",
    "        plt.show()\n",
    "del cluster_coeff_final, cluster_coeff_original, df, reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input vs Output vs Sys rates Rates over time\n",
    "\"\"\"\n",
    "# For logic\n",
    "for label, reader in dataset.items():\n",
    "    states = reader['step_states']\n",
    "    inrates = reader['step_inrates']\n",
    "    sysrates = reader['step_sysrates']\n",
    "    outrates = reader['step_outrates']\n",
    "\n",
    "    for state in [[False, False], [False, True], [True, False], [True, True]]:\n",
    "        plt.scatter(np.where(np.all(states == state, axis=1), outrates, -1))\n",
    "\n",
    "\n",
    "        plt.title(state)\n",
    "        plt.show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input rates vs Output rates scatter -- last len_episodes\n",
    "N_PLOTS = 5\n",
    "\n",
    "for label, reader in dataset.items():\n",
    "    accuracies = reader['accuracy']\n",
    "    top_accuracies = np.argsort(accuracies).values[-N_PLOTS:]\n",
    "\n",
    "    inrates = reader['step_inrates', top_accuracies]\n",
    "    outrates = reader['step_outrates', top_accuracies]\n",
    "    titles = [f\"{label} - {accuracy}\" for accuracy in accuracies[top_accuracies]]\n",
    "\n",
    "    outrates_scatter(inrates, outrates, int(reader['len_episode', 1][0]), N_PLOTS, titles=titles)\n",
    "\n",
    "del reader, accuracies, top_accuracies, inrates, outrates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input rates vs Output rates scatter -- last len_episodes\n",
    "N_PLOTS = 5\n",
    "\n",
    "for label, reader in dataset.items():\n",
    "    accuracies = reader['accuracy']\n",
    "    low_accuracies = np.argsort(accuracies).values[:N_PLOTS]\n",
    "\n",
    "    inrates = reader['step_inrates', low_accuracies]\n",
    "    outrates = reader['step_outrates', low_accuracies]\n",
    "    titles = [f\"{label} - {accuracy}\" for accuracy in accuracies[low_accuracies]]\n",
    "\n",
    "    outrates_scatter(inrates, outrates, int(reader['len_episode', 1][0]), N_PLOTS, titles=titles)\n",
    "del reader, accuracies, low_accuracies, inrates, outrates, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input rates vs Output rates scatter -- last len_episodes\n",
    "N_PLOTS = 5\n",
    "\n",
    "for label, reader in dataset.items():\n",
    "    accuracies = reader['accuracy']\n",
    "    random_accuracies = np.int_(np.arange(0, N_PLOTS))\n",
    "\n",
    "    inrates = reader['step_inrates', random_accuracies]\n",
    "    outrates = reader['step_outrates', random_accuracies]\n",
    "    titles = [f\"{label} - {accuracy}\" for accuracy in accuracies[random_accuracies]]\n",
    "\n",
    "    outrates_scatter(inrates, outrates, int(reader['len_episode', 1][0]), N_PLOTS, titles=titles)\n",
    "del reader, accuracies, low_accuracies, inrates, outrates, titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "0b662b53-0077-4ef3-b548-f606fd57e5ac",
   "display_name": "'Python Interactive'"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}